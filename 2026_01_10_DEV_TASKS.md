# SEOman Development Tasks - 2026-01-10

## Overview

This document outlines the remaining development tasks to make SEOman a complete, production-ready SEO platform. Tasks are organized by priority and include implementation details, dependencies, and effort estimates.

**Current State**: SEOman v2.0 with 100-check audit engine, crawler, keyword research, and content planning.

### Progress Summary (2026-01-10)

| Category | Completed | Remaining |
|----------|-----------|-----------|
| P0 Critical | **4/4 (100%)** | ✅ All Complete! |
| P1 High | 0/4 | All pending |
| P2 Medium | 0/4 | All pending |
| P3 Low | 0/6 | All pending |

**Recently Completed:**
- ✅ Rank Tracking - Full implementation with SERP features, historical data, and API endpoints
- ✅ JS Rendering - Playwright integration with SPA detection and batch rendering
- ✅ API Rate Limiting - Redis-based rate limiting with quota enforcement and usage tracking
- ✅ Test Suite - Comprehensive pytest suite with unit and integration tests

---

## Priority Legend

- **P0** - Critical: Must have for production launch
- **P1** - High: Essential for competitive SEO tool
- **P2** - Medium: Important for user experience
- **P3** - Low: Nice to have / Future roadmap

**Effort Scale**: S (1-2 days), M (3-5 days), L (1-2 weeks), XL (2-4 weeks)

---

## P0 - Critical Tasks

### 1. ~~Complete Rank Tracking Implementation~~ ✅ COMPLETED

**Status**: ✅ Implemented (2026-01-10)
**Effort**: L
**Files modified**:
- `backend/app/tasks/keyword_tasks.py`
- `backend/app/services/keyword_service.py`
- `backend/app/models/keyword.py`
- `backend/app/api/v1/keywords.py`
- `backend/app/schemas/keyword.py`
- `backend/app/integrations/dataforseo.py`

#### Completed Tasks:

- [x] **1.1** Create `KeywordRanking` model for historical data
  - Added `KeywordRanking` model with position, URL, SERP features, competitor positions
  - Added tracking fields to `Keyword` model (is_tracked, current_position, previous_position, best_position, ranking_url, last_checked_at)

- [x] **1.2** Implement `update_keyword_rankings` task in `keyword_tasks.py`
  - Integrated with DataForSEO SERP API via `get_serp_with_features()` and `get_rankings_batch()`
  - Stores historical positions in `keyword_rankings` table
  - Tracks SERP feature presence (featured snippets, PAA, local pack, etc.)
  - Batch processing with rate limiting

- [x] **1.3** Create ranking history API endpoints
  - `GET /sites/{site_id}/rankings` - List tracked keywords with current rankings
  - `GET /sites/{site_id}/rankings/summary` - Overview with statistics
  - `GET /sites/{site_id}/rankings/changes` - Biggest movers (gainers/losers)
  - `GET /keywords/{keyword_id}/rankings` - Historical data
  - `POST /sites/{site_id}/rankings/update` - Trigger ranking update
  - `POST /sites/{site_id}/keywords/tracking` - Enable/disable tracking
  - `POST /sites/{site_id}/keywords/track-top` - Auto-track top N keywords

- [x] **1.4** Add ranking change detection
  - Calculates position changes (previous vs current)
  - Tracks best position ever achieved
  - `get_ranking_changes()` returns biggest gainers and losers

- [ ] **1.5** Frontend: Ranking dashboard component (Pending frontend implementation)
  - Position trend charts
  - SERP feature indicators
  - Competitor comparison view

---

### 2. ~~JavaScript Rendering Support (Headless Browser)~~ ✅ COMPLETED

**Status**: ✅ Implemented (2026-01-10)
**Effort**: M
**Files created/modified**:
- `backend/app/services/js_crawler.py` (new)
- `backend/app/services/crawler.py`
- `backend/app/models/crawl.py`
- `backend/Dockerfile`
- `backend/requirements.txt`

#### Completed Tasks:

- [x] **2.1** Add Playwright to backend dependencies
  - Added `playwright==1.40.0` to requirements.txt

- [x] **2.2** Update Docker configuration
  - Added Playwright installation stages to Dockerfile
  - Created variants: `development`, `development-light`, `production`, `production-light`
  - Light variants skip Playwright for resource-constrained environments

- [x] **2.3** Create `JSCrawler` service in `backend/app/services/js_crawler.py`
  - `render_page()` - Render single page with headless Chromium
  - `render_batch()` - Batch rendering with concurrency control
  - Waits for network idle, extracts rendered HTML
  - Captures console errors and resource timing
  - Graceful fallback when Playwright not available

- [x] **2.4** Integrate with main crawler
  - Added `js_rendering` config options to `CrawlConfig`
  - Added JS rendering fields to `CrawledPage` dataclass
  - Two-phase rendering: auto-detect SPAs during crawl, batch re-render
  - `_check_needs_js_rendering()` for automatic detection
  - Resource limits via `max_concurrent_renders` setting

- [x] **2.5** Add SPA detection heuristic
  - `detect_spa_from_html()` - Detects React, Vue, Angular, Svelte, Ember, Next.js, Nuxt, Gatsby
  - Checks for framework signatures, empty content with JS bundles
  - `should_use_js_rendering()` - Decides based on word count and framework detection
  - Added `spa_detected` and `framework_detected` fields to `CrawlPage` model

---

### 3. ~~API Rate Limiting & Usage Quotas~~ ✅ COMPLETED

**Status**: ✅ Implemented (2026-01-10)
**Effort**: M
**Files created/modified**:
- `backend/app/models/usage.py` (new)
- `backend/app/services/rate_limiter.py` (new)
- `backend/app/core/rate_limit_middleware.py` (new)
- `backend/app/core/deps.py`
- `backend/app/api/v1/usage.py` (new)
- `backend/app/schemas/usage.py` (new)
- `backend/app/config.py`
- `backend/app/main.py`

#### Completed Tasks:

- [x] **3.1** Create usage tracking models
  - `TenantUsage` - Monthly usage tracking per tenant (api_calls, pages_crawled, keywords_researched, audits_run, content_generated, js_renders)
  - `TenantQuota` - Custom quota overrides per tenant
  - `RateLimitEvent` - Logging of rate limit/quota exceeded events
  - `UsageType` enum for different trackable metrics

- [x] **3.2** Implement rate limiter middleware
  - Redis-based sliding window algorithm in `RateLimiter` class
  - `RateLimitMiddleware` for automatic rate limiting
  - Returns `429 Too Many Requests` with `Retry-After` header
  - Rate limit headers on all responses (X-RateLimit-Limit, X-RateLimit-Remaining, X-RateLimit-Reset)
  - Configurable exempt paths and custom limits per endpoint

- [x] **3.3** Add plan-based quotas
  - `PLAN_QUOTAS` configuration for free, pro, enterprise plans
  - Configurable via environment variables (QUOTA_FREE_*, QUOTA_PRO_*, QUOTA_ENTERPRISE_*)
  - Enterprise plan has unlimited quotas (0 = unlimited)
  - Plan-specific rate limits per minute

- [x] **3.4** Create quota enforcement dependencies
  - `require_quota(usage_type, amount)` - Dependency factory for quota enforcement
  - `track_usage(usage_type, amount)` - Track without blocking
  - Convenience dependencies: `RequireCrawlQuota`, `RequireAuditQuota`, `RequireKeywordQuota`, `RequireContentQuota`, `RequireJSRenderQuota`
  - Returns `402 Payment Required` with upgrade URL when quota exceeded

- [x] **3.5** Add usage dashboard API
  - `GET /usage` - Current period usage summary
  - `GET /usage/history` - Historical usage (past N months)
  - `GET /usage/quotas` - Plan quota limits
  - `GET /usage/rate-limit` - Current rate limit status
  - `GET /usage/tenants/{id}` - Admin: get tenant usage
  - `PATCH /usage/tenants/{id}/quotas` - Admin: update tenant quotas
  - `DELETE /usage/tenants/{id}/quotas` - Admin: reset to plan defaults

---

### 4. ~~Test Suite Implementation~~ ✅ COMPLETED

**Status**: ✅ Implemented (2026-01-10)
**Effort**: XL
**Files created**:
- `backend/tests/conftest.py`
- `backend/tests/unit/test_audit_engine.py`
- `backend/tests/unit/test_crawler.py`
- `backend/tests/unit/test_rate_limiter.py`
- `backend/tests/unit/test_keyword_service.py`
- `backend/tests/unit/test_scoring.py`
- `backend/tests/integration/test_api_auth.py`
- `backend/tests/integration/test_api_sites.py`
- `backend/tests/integration/test_api_usage.py`
- `backend/tests/fixtures/sample_pages.py`
- `backend/pytest.ini`

#### Completed Tasks:

- [x] **4.1** Set up pytest infrastructure
  - Created `pytest.ini` with async mode configuration
  - Added test markers (unit, integration, slow)
  - Configured test discovery and reporting

- [x] **4.2** Create test fixtures
  - SQLite in-memory test database with async support
  - Mock Redis client for rate limiting tests
  - Mock external services (DataForSEO, storage, httpx)
  - Sample HTML pages and crawl data fixtures
  - Authentication token fixtures

- [x] **4.3** Unit tests for core services
  - Audit engine: Tests for all 10 categories (100 checks)
  - Scoring algorithm validation with severity weighting
  - Crawler service: URL handling, HTML extraction, link discovery
  - Rate limiter: Redis operations, quota checks, rate limiting
  - Keyword service: Ranking models, metrics, DataForSEO integration

- [x] **4.4** Integration tests for API endpoints
  - Authentication: Token validation, role-based access
  - Sites API: CRUD operations, validation
  - Usage API: Quota and rate limit endpoints

- [ ] **4.5** Add CI/CD pipeline (Pending)
  - GitHub Actions workflow
  - Run tests on PR
  - Coverage reporting

**Run tests with:**
```bash
cd backend && pytest -v
cd backend && pytest -v --cov=app --cov-report=html
```

---

## P1 - High Priority Tasks

### 5. Backlink Analysis Integration

**Status**: Not implemented
**Effort**: XL
**New files**:
- `backend/app/integrations/backlinks.py`
- `backend/app/models/backlink.py`
- `backend/app/services/backlink_service.py`

#### Tasks:

- [ ] **5.1** Choose and integrate backlink data provider
  - Option A: Ahrefs API (comprehensive, expensive)
  - Option B: Moz Link Explorer API (good balance)
  - Option C: Majestic API (affordable)
  - Option D: DataForSEO Backlinks API (already integrated)

- [ ] **5.2** Create backlink models
  ```python
  class Backlink(Base):
      id: UUID
      site_id: UUID (FK)
      source_url: str
      target_url: str
      anchor_text: str
      link_type: Enum  # dofollow, nofollow, ugc, sponsored
      first_seen: datetime
      last_seen: datetime
      is_lost: bool
      domain_rating: float
      page_authority: float

  class BacklinkProfile(Base):
      site_id: UUID (FK)
      total_backlinks: int
      referring_domains: int
      domain_rating: float
      trust_flow: float
      citation_flow: float
      updated_at: datetime
  ```

- [ ] **5.3** Implement backlink discovery service
  - Fetch new backlinks periodically
  - Track lost backlinks
  - Identify toxic/spammy links

- [ ] **5.4** Create competitor backlink comparison
  - Gap analysis: links competitors have that you don't
  - Common backlink sources
  - Link velocity comparison

- [ ] **5.5** Add backlink API endpoints
  - `GET /sites/{site_id}/backlinks` - List with filters
  - `GET /sites/{site_id}/backlinks/profile` - Overview metrics
  - `GET /sites/{site_id}/backlinks/new` - Recently acquired
  - `GET /sites/{site_id}/backlinks/lost` - Recently lost
  - `POST /sites/{site_id}/backlinks/disavow` - Mark for disavow

- [ ] **5.6** Toxic link detection
  - Spam score calculation
  - Link pattern analysis
  - Disavow file generation

---

### 6. Real-Time Monitoring & Alerts

**Status**: Not implemented
**Effort**: L
**New files**:
- `backend/app/models/alert.py`
- `backend/app/services/alert_service.py`
- `backend/app/tasks/monitoring_tasks.py`

#### Tasks:

- [ ] **6.1** Create alert configuration model
  ```python
  class AlertRule(Base):
      id: UUID
      site_id: UUID (FK)
      alert_type: Enum  # ranking_drop, site_down, index_change, score_drop
      threshold: JSON  # {"positions": 5, "timeframe": "24h"}
      channels: JSON  # ["email", "slack", "webhook"]
      is_active: bool

  class AlertEvent(Base):
      id: UUID
      rule_id: UUID (FK)
      triggered_at: datetime
      data: JSON  # Context about the alert
      acknowledged: bool
      acknowledged_by: UUID (FK User)
  ```

- [ ] **6.2** Implement monitoring tasks
  - Site uptime check (every 5 min)
  - Ranking position monitoring (daily)
  - Google index status check (daily)
  - Audit score monitoring (after each audit)

- [ ] **6.3** Create notification service
  - Email notifications (SMTP/SendGrid)
  - Slack webhook integration
  - Generic webhook for integrations

- [ ] **6.4** Add alert management API
  - `POST /sites/{site_id}/alerts` - Create rule
  - `GET /sites/{site_id}/alerts` - List rules
  - `GET /sites/{site_id}/alerts/events` - Alert history
  - `POST /alerts/events/{event_id}/acknowledge` - Acknowledge

- [ ] **6.5** Frontend: Alert configuration UI
  - Rule builder with conditions
  - Channel configuration
  - Alert history timeline

---

### 7. PageSpeed Insights Integration

**Status**: Partial (basic CWV checks only)
**Effort**: M
**Files to modify**:
- `backend/app/integrations/` (new file)
- `backend/app/services/audit_engine.py`

#### Tasks:

- [ ] **7.1** Create PageSpeed Insights client
  ```python
  # backend/app/integrations/pagespeed.py
  class PageSpeedInsightsClient:
      async def analyze(url: str, strategy: str = "mobile") -> PSIResult:
          # Call PSI API
          # Parse Lighthouse metrics
          # Extract opportunities and diagnostics
  ```

- [ ] **7.2** Add performance history model
  ```python
  class PerformanceSnapshot(Base):
      site_id: UUID (FK)
      url: str
      strategy: Enum  # mobile, desktop
      performance_score: int
      lcp: float
      fid: float  # or INP
      cls: float
      ttfb: float
      opportunities: JSON
      diagnostics: JSON
      checked_at: datetime
  ```

- [ ] **7.3** Enhance audit engine with PSI data
  - Deep performance analysis per page
  - Specific optimization recommendations
  - Resource-level insights (JS, CSS, images)

- [ ] **7.4** Create performance tracking endpoints
  - `POST /sites/{site_id}/performance/analyze` - Run PSI
  - `GET /sites/{site_id}/performance/history` - Trend data
  - `GET /sites/{site_id}/performance/opportunities` - Aggregated fixes

- [ ] **7.5** Add Lighthouse CI for scheduled monitoring
  - Track performance over time
  - Detect regressions
  - Performance budgets

---

### 8. PDF Report Generation

**Status**: Markdown only
**Effort**: M
**Dependencies**: WeasyPrint or Puppeteer

#### Tasks:

- [ ] **8.1** Choose PDF generation approach
  - Option A: WeasyPrint (HTML/CSS to PDF, Python native)
  - Option B: Puppeteer/Playwright (Chrome-based, better rendering)
  - Option C: External service (DocRaptor, PDFShift)

- [ ] **8.2** Create report templates
  ```
  backend/app/templates/reports/
  ├── base.html
  ├── audit_report.html
  ├── seo_plan.html
  ├── content_brief.html
  └── styles/
      ├── report.css
      └── charts.css
  ```

- [ ] **8.3** Implement PDF generator service
  ```python
  class PDFGenerator:
      async def generate_audit_report(audit_id: UUID) -> bytes
      async def generate_plan_report(plan_id: UUID) -> bytes
      async def generate_brief_pdf(brief_id: UUID) -> bytes
  ```

- [ ] **8.4** Add charts and visualizations
  - Score gauges
  - Issue distribution pie charts
  - Trend line graphs
  - Competitor comparison tables

- [ ] **8.5** White-label support
  - Custom logo upload
  - Brand color configuration
  - Custom cover pages
  - Footer customization

- [ ] **8.6** Add PDF endpoints
  - `GET /audits/{audit_id}/pdf` - Download PDF report
  - `GET /plans/{plan_id}/pdf` - Download plan PDF
  - `POST /reports/generate` - Generate custom report

---

## P2 - Medium Priority Tasks

### 9. Competitive Analysis Enhancement

**Status**: Stubbed
**Effort**: L

#### Tasks:

- [ ] **9.1** Create competitor model
  ```python
  class Competitor(Base):
      id: UUID
      site_id: UUID (FK)  # Your site
      competitor_domain: str
      is_auto_detected: bool
      tracked_since: datetime

  class CompetitorMetrics(Base):
      competitor_id: UUID (FK)
      domain_authority: float
      organic_traffic_estimate: int
      keyword_overlap: int
      backlink_count: int
      updated_at: datetime
  ```

- [ ] **9.2** Implement competitor discovery
  - SERP overlap analysis
  - Industry/niche detection
  - Manual competitor addition

- [ ] **9.3** Build comparison dashboards
  - Side-by-side metrics
  - Keyword gap visualization
  - Backlink gap analysis
  - Content gap identification

- [ ] **9.4** Create competitor tracking tasks
  - Weekly metrics update
  - New keyword alerts
  - Content publication monitoring

---

### 10. Schema Markup Builder

**Status**: Validation only
**Effort**: M

#### Tasks:

- [ ] **10.1** Create schema template library
  - Article, BlogPosting, NewsArticle
  - Product, Offer, AggregateRating
  - LocalBusiness, Organization
  - FAQ, HowTo, Recipe
  - Event, Course, Video

- [ ] **10.2** Implement schema generator service
  - Template-based generation
  - Page content extraction
  - Automatic field population

- [ ] **10.3** Add schema recommendation engine
  - Analyze page type
  - Suggest applicable schemas
  - Validate generated markup

- [ ] **10.4** Create schema builder API
  - `GET /schema/templates` - Available templates
  - `POST /schema/generate` - Generate from content
  - `POST /schema/validate` - Validate markup

---

### 11. Scheduled Report Delivery

**Status**: Not implemented
**Effort**: M

#### Tasks:

- [ ] **11.1** Create report schedule model
  ```python
  class ReportSchedule(Base):
      site_id: UUID (FK)
      report_type: Enum  # audit_summary, ranking_update, weekly_digest
      frequency: Enum  # daily, weekly, monthly
      recipients: JSON  # List of emails
      last_sent: datetime
      next_run: datetime
  ```

- [ ] **11.2** Implement email delivery service
  - SMTP configuration
  - SendGrid/Mailgun integration
  - HTML email templates

- [ ] **11.3** Create Celery beat tasks
  - Check for due reports
  - Generate and send
  - Track delivery status

- [ ] **11.4** Add schedule management API
  - `POST /sites/{site_id}/report-schedules`
  - `GET /sites/{site_id}/report-schedules`
  - `DELETE /report-schedules/{schedule_id}`

---

### 12. Content Optimization Suggestions

**Status**: Basic briefs only
**Effort**: M

#### Tasks:

- [ ] **12.1** Implement on-page content analyzer
  - Keyword density analysis
  - Semantic keyword suggestions
  - Readability scoring (Flesch-Kincaid)
  - Content length recommendations

- [ ] **12.2** Create SERP-based content analysis
  - Analyze top 10 ranking pages
  - Extract common topics/entities
  - Identify content gaps
  - Word count benchmarking

- [ ] **12.3** Add real-time content scoring
  - As-you-type optimization hints
  - Target keyword tracking
  - Structure recommendations

---

## P3 - Low Priority / Future Roadmap

### 13. Local SEO Features

**Effort**: XL

- [ ] Google Business Profile API integration
- [ ] Local citation tracking (Yext, BrightLocal data)
- [ ] NAP consistency checker
- [ ] Local pack rank tracking
- [ ] Review monitoring and sentiment analysis
- [ ] Local schema generation

---

### 14. Log File Analysis

**Effort**: XL

- [ ] Log file upload/parsing (Apache, Nginx, Cloudflare)
- [ ] Bot activity analysis (Googlebot, Bingbot, etc.)
- [ ] Crawl budget analysis
- [ ] Orphan page detection
- [ ] Crawl frequency insights
- [ ] Status code distribution over time

---

### 15. International SEO Tools

**Effort**: L

- [ ] Hreflang validator and generator
- [ ] Multi-region SERP tracking
- [ ] Geo-targeting analysis
- [ ] International keyword research
- [ ] ccTLD vs subdomain analysis

---

### 16. AI Content Detection

**Effort**: M

- [ ] Integration with AI detection APIs (Originality.ai, GPTZero)
- [ ] Content authenticity scoring
- [ ] Competitor AI content analysis
- [ ] Recommendations for humanization

---

### 17. WordPress/CMS Integrations

**Effort**: L per integration

- [ ] WordPress plugin for direct publishing
- [ ] Shopify integration
- [ ] Webflow integration
- [ ] API-first CMS connections (Contentful, Sanity)

---

### 18. Advanced Analytics Integration

**Effort**: M

- [ ] Google Analytics 4 connection
- [ ] Google Search Console integration
- [ ] Traffic correlation with SEO changes
- [ ] Conversion tracking per keyword

---

## Technical Debt & Improvements

### 19. Code Quality

- [ ] Add comprehensive docstrings to all services
- [ ] Implement proper error codes enum
- [ ] Add request validation for all endpoints
- [ ] Resolve circular import issues in tasks
- [ ] Add structured logging with correlation IDs

### 20. Performance Optimization

- [ ] Add database query optimization (N+1 detection)
- [ ] Implement Redis caching for frequent queries
- [ ] Add connection pooling tuning
- [ ] Optimize large crawl job handling

### 21. Security Hardening

- [ ] Add input sanitization across all endpoints
- [ ] Implement CSRF protection
- [ ] Add security headers middleware
- [ ] Audit for OWASP Top 10 vulnerabilities
- [ ] Add secrets scanning in CI/CD

### 22. Documentation

- [ ] API documentation (OpenAPI/Swagger)
- [ ] Developer onboarding guide
- [ ] Architecture decision records (ADRs)
- [ ] Deployment runbook
- [ ] User documentation/help center

---

## Implementation Phases

### Phase 1: Production Ready (4-6 weeks) ✅ COMPLETE
Focus: P0 tasks
- ✅ Complete rank tracking (DONE - 2026-01-10)
- ✅ Add JS rendering (DONE - 2026-01-10)
- ✅ Implement rate limiting (DONE - 2026-01-10)
- ✅ Build test suite (DONE - 2026-01-10)

### Phase 2: Competitive Feature Parity (6-8 weeks)
Focus: P1 tasks
- Backlink analysis
- Real-time alerts
- PageSpeed integration
- PDF reports

### Phase 3: Advanced Features (8-12 weeks)
Focus: P2 tasks
- Competitive analysis
- Schema builder
- Scheduled reports
- Content optimization

### Phase 4: Market Differentiation (Ongoing)
Focus: P3 tasks
- Local SEO
- Log analysis
- CMS integrations
- Advanced analytics

---

## Dependencies & External Services

| Service | Purpose | Status | Required For |
|---------|---------|--------|--------------|
| DataForSEO | Keywords, SERP, Backlinks | ✅ Integrated | Rank tracking, backlinks |
| Redis | Rate limiting, caching | ✅ Integrated | Rate limiting, quotas |
| Playwright | JS rendering | ✅ Integrated | SPA crawling |
| PageSpeed Insights | Performance analysis | Not integrated | Performance monitoring |
| SendGrid/Mailgun | Email delivery | Not integrated | Alerts, scheduled reports |
| Ahrefs/Moz API | Backlink data | Not integrated | Backlink analysis |
| Google APIs | GSC, GA4, GBP | Not integrated | Analytics, local SEO |

---

## Notes

- All effort estimates assume a single senior developer
- External API costs should be evaluated before integration
- Some P1 features may be promoted to P0 based on user feedback
- Regular backlog grooming recommended every 2 weeks

---

*Document created: 2026-01-10*
*Last updated: 2026-01-10 (Phase 1 Complete: Rank Tracking, JS Rendering, Rate Limiting, Test Suite)*
